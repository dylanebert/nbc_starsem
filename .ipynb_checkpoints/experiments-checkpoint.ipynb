{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import sys\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "sys.path.append('/media/dylan/Elements/') #change to path containing nbc repository\n",
    "from nbc.nbc import NBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "Requires nbc repository: https://github.com/dylanebert/nbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "NBC.add_args(parser)\n",
    "args = parser.parse_args([\n",
    "    '--subsample', '1',\n",
    "    '--dynamic_only', 'True',\n",
    "    '--train_sequencing', 'token_aligned',\n",
    "    '--test_sequencing', 'chunked',\n",
    "    '--features',\n",
    "    'dist_to_head:most_moving',\n",
    "    'avg_vel:most_moving',\n",
    "    'var_vel:most_moving',\n",
    "    'traj:most_moving',\n",
    "    'avg_rel:most_moving'\n",
    "])\n",
    "nbc = NBC(args)\n",
    "vgg_embeddings = nbc.get_vgg_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess vgg embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = {'train': {}, 'test': {}}\n",
    "for type in ['train', 'test']:\n",
    "    for k, z in vgg_embeddings[type].items():\n",
    "        z = z[np.arange(0, z.shape[0], 9)]\n",
    "        start = z[0]; end = z[-1]\n",
    "        mean = np.mean(z, axis=0); var = np.var(z, axis=0)\n",
    "        peak = np.max(z, axis=0); trough = np.min(z, axis=0)\n",
    "        vec = np.concatenate([start, end, mean, var, peak, trough])\n",
    "        vgg[type][k] = vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute predicted token for each test instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbors(model, features, supervised):    \n",
    "    train_x = np.vstack(list(features['train'].values()))\n",
    "    test_x = np.vstack(list(features['test'].values()))\n",
    "    \n",
    "    train_tokens = np.array([key[2] for key in features['train'].keys()])\n",
    "    vocab = {v: k for k, v in enumerate(np.unique(train_tokens))}\n",
    "    train_y = np.array([vocab[v] for v in train_tokens])\n",
    "    \n",
    "    if supervised:\n",
    "        reducer = LinearDiscriminantAnalysis(n_components=10)\n",
    "        reducer.fit(train_x, train_y)\n",
    "    else:\n",
    "        reducer = TruncatedSVD(n_components=10)\n",
    "        reducer.fit(train_x)\n",
    "        \n",
    "    train_x_reduced = reducer.transform(train_x)\n",
    "    test_x_reduced = reducer.transform(test_x)\n",
    "    \n",
    "    token_means = {k: [] for k in vocab.keys()}\n",
    "    for i, key in enumerate(features['train'].keys()):\n",
    "        token = key[2]\n",
    "        token_means[token].append(train_x_reduced[i])\n",
    "    for k in token_means.keys():\n",
    "        token_means[k] = np.mean(token_means[k], axis=0)\n",
    "    token_means_mat = np.vstack(list(token_means.values()))\n",
    "    \n",
    "    results = []\n",
    "    for key, feat in zip(features['test'].keys(), test_x_reduced):\n",
    "        dist = cosine_similarity(feat.reshape((1, -1)), np.vstack(token_means_mat))[0]\n",
    "        indices = np.argsort(dist)[::-1]\n",
    "        tokens = np.array(list(token_means.keys()))[indices]\n",
    "        dists = dist[indices]\n",
    "        res = {\n",
    "            'model': model,\n",
    "            'clip': '{} {}-{}'.format(key[0], key[1], key[1] + 450),\n",
    "            'predicted': tokens[0]\n",
    "        }\n",
    "        for i, (token, p) in enumerate(zip(tokens, dists)):\n",
    "            res['r{}'.format(i+1)] = '{}={:.4f}'.format(token, p)\n",
    "        results.append(res)\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def random_neighbors():\n",
    "    tokens = np.unique(np.array([key[2] for key in features['train'].keys()]))    \n",
    "    results = []\n",
    "    for key in nbc.features['test'].keys():\n",
    "        indices = np.arange(0, len(tokens))\n",
    "        np.random.shuffle(indices)\n",
    "        tokens = tokens[indices]\n",
    "        res = {\n",
    "            'model': 'rand',\n",
    "            'clip': '{} {}-{}'.format(key[0], key[1], key[1] + 450),\n",
    "            'predicted': tokens[0]\n",
    "        }\n",
    "        results.append(res)\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run on spatial and cnn models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for model, features, supervised in zip(['spatial_sup', 'spatial_unsup', 'cnn_sup', 'cnn_unsup'],\n",
    "                                      [nbc.features, nbc.features, vgg, vgg],\n",
    "                                      [True, False, True, False]):\n",
    "    results.append(nearest_neighbors(model, features, supervised))\n",
    "results.append(random_neighbors())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add in spatial with names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del nbc #may need to free memory\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "NBC.add_args(parser)\n",
    "args = parser.parse_args([\n",
    "    '--subsample', '1',\n",
    "    '--dynamic_only', 'True',\n",
    "    '--train_sequencing', 'token_aligned',\n",
    "    '--test_sequencing', 'chunked',\n",
    "    '--features',\n",
    "    'one_hot_label:most_moving', #add in one-hot encoding of label of most moving object\n",
    "    'dist_to_head:most_moving',\n",
    "    'avg_vel:most_moving',\n",
    "    'var_vel:most_moving',\n",
    "    'traj:most_moving',\n",
    "    'avg_rel:most_moving'\n",
    "])\n",
    "nbc_with_names = NBC(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append(nearest_neighbors('spatial_with_names_sup', nbc_with_names.features, True))\n",
    "results.append(nearest_neighbors('spatial_with_names_unsup', nbc_with_names.features, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine results into single csv for annotation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat(results).sample(frac=1)\n",
    "results = results.sort_values(by='clip')\n",
    "results.to_csv('neighbors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
